{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neural_network import NeuralNetwork\n",
    "from dense_layer import DenseLayer\n",
    "from conv_layer import Convolutional2D\n",
    "from reshape import Reshape\n",
    "from maxpooling import MaxPooling2D\n",
    "from activations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"mnist_train.csv\")\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "X_dev = X_dev.T\n",
    "\n",
    "data_train = data[1000:7000].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "X_train = X_train.T\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = X_train.reshape(-1, 28, 28)\n",
    "\n",
    "# Step 2: Add padding to convert 28x28 to 32x32\n",
    "X_train_padded = np.pad(X_train_reshaped, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
    "\n",
    "# Step 3 (optional): Flatten the padded images back to 1D (if needed)\n",
    "X_train = X_train_padded.reshape(-1, 32*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_reshaped = X_dev.reshape(-1, 28, 28)\n",
    "\n",
    "# Step 2: Add padding to convert 28x28 to 32x32\n",
    "X_dev_padded = np.pad(X_dev_reshaped, ((0, 0), (2, 2), (2, 2)), 'constant')\n",
    "\n",
    "# Step 3 (optional): Flatten the padded images back to 1D (if needed)\n",
    "X_dev = X_dev_padded.reshape(-1, 32*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "class Convolutional2D:\n",
    "    def __init__(self, kernel_size, num_kernels, padding=0, stride=1, kernels=None):\n",
    "        self.input_shape = None\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_kernels = num_kernels\n",
    "        self.biases = np.random.randn(num_kernels)\n",
    "        self.kernels = kernels\n",
    "    \n",
    "    def _init_params(self, input_shape):\n",
    "        self.input_shape = input_shape[1:]\n",
    "        input_height, input_width, input_channels = self.input_shape[0], self.input_shape[1], self.input_shape[2]\n",
    "        self.output_height = (input_height + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        self.output_width = (input_width + 2 * self.padding - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        self.kernels_shape = (self.kernel_size, self.kernel_size, input_channels, self.num_kernels)\n",
    "        if self.kernels is None:\n",
    "            self.kernels = np.random.randn(self.kernel_size, self.kernel_size, input_channels, self.num_kernels) * 0.01\n",
    "\n",
    "    def _pad_input(self, input):\n",
    "        if self.padding > 0:\n",
    "            padded_input = np.pad(\n",
    "                input,\n",
    "                ((0, 0),\n",
    "                 (self.padding, self.padding),\n",
    "                 (self.padding, self.padding),\n",
    "                 (0, 0)),\n",
    "                mode='constant'\n",
    "            )\n",
    "            return padded_input\n",
    "        return input\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.input_shape is None:\n",
    "            self._init_params(input.shape)\n",
    "        self.batch_size = input.shape[0]\n",
    "\n",
    "        self.input = self._pad_input(input)\n",
    "        self.output = np.zeros((self.batch_size, self.output_height, self.output_width, self.num_kernels))\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            for k in range(self.num_kernels):\n",
    "                conv_result = np.sum([\n",
    "                    signal.correlate2d(self.input[i, :, :, c], self.kernels[:, :, c, k], mode='valid')\n",
    "                    for c in range(self.input.shape[3])\n",
    "                ], axis=0)\n",
    "                self.output[i, :, :, k] = conv_result[::self.stride, ::self.stride] + self.biases[k]\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros_like(self.kernels)\n",
    "        input_gradient = np.zeros_like(self.input)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            for k in range(self.num_kernels):\n",
    "                upsampled_gradient = np.zeros((\n",
    "                    (output_gradient.shape[1] - 1) * self.stride + 1,\n",
    "                    (output_gradient.shape[2] - 1) * self.stride + 1\n",
    "                ))\n",
    "                upsampled_gradient[::self.stride, ::self.stride] = output_gradient[i, :, :, k]\n",
    "                \n",
    "                for c in range(self.input.shape[3]):\n",
    "                    kernels_gradient[:, :, c, k] += signal.correlate2d(\n",
    "                        self.input[i, :, :, c], upsampled_gradient, mode='valid'\n",
    "                    )\n",
    "                    input_gradient[i, :, :, c] += signal.convolve2d(\n",
    "                        upsampled_gradient, self.kernels[:, :, c, k], mode='full'\n",
    "                    )\n",
    "        \n",
    "        self.kernels -= learning_rate * kernels_gradient / self.batch_size\n",
    "        self.biases -= learning_rate * np.sum(output_gradient, axis=(0, 1, 2)) / self.batch_size\n",
    "        \n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from loss_function import LossFunction\n",
    "\n",
    "# ANN Class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss_functions = {\n",
    "            'mse': (LossFunction.mse, LossFunction.mse_derivative),\n",
    "            'log_loss': (LossFunction.log_loss, LossFunction.log_loss_derivative)\n",
    "        }\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def compile(self, loss):\n",
    "        self.loss = loss\n",
    "        if loss not in self.loss_functions:\n",
    "            raise ValueError(\"Loss function not supported.\")\n",
    "        self.loss_function, self.loss_derivative = self.loss_functions[loss]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output):\n",
    "        for layer in reversed(self.layers):\n",
    "            output = layer.backward(output, self.learning_rate)\n",
    "    \n",
    "    def train(self, X, Y, epochs = 10000, learning_rate = 0.1, batch_size = 50, softmax_logloss=False, isOne_hot=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        n_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "        y = Y.copy()\n",
    "        if isOne_hot:\n",
    "            Y = one_hot(Y)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_error = 0\n",
    "            for batch_idx in range(n_batches):\n",
    "                start = batch_idx * batch_size\n",
    "                end = min(start + batch_size, X.shape[0])\n",
    "                \n",
    "                X_batch = X[start:end]\n",
    "                y_batch = Y[start:end]\n",
    "\n",
    "                # Forward pass for the entire batch\n",
    "                output = self.forward(X_batch)\n",
    "                \n",
    "                # Calculate batch loss and gradients\n",
    "                error = self.loss_function(y_batch, output)\n",
    "                if softmax_logloss:\n",
    "                    grad = (output - y_batch) / batch_size\n",
    "                else:\n",
    "                    grad = self.loss_derivative(y_batch, output)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(grad)\n",
    "                \n",
    "                total_error += error\n",
    "            total_error /= n_batches\n",
    "            output = self.forward(X)\n",
    "            pred = get_predictions(output)\n",
    "            print(f'Epoch {epoch}, Loss: {total_error}')\n",
    "            if (one_hot):\n",
    "                print(f'Epoch {epoch}, accuracy: {get_accuracy(pred, y)}')\n",
    "            # if (epoch % 10 == 0):\n",
    "            #     output = self.forward(X)\n",
    "            #     pred = get_predictions(output)\n",
    "            #     print(f'Epoch {epoch}, Loss: {total_error}')\n",
    "            #     if (one_hot):\n",
    "            #         print(f'Epoch {epoch}, accuracy: {get_accuracy(pred, y)}')\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "def print_parameters(layer, num_elements=5):\n",
    "    print(f\"First {num_elements} weights: {layer.weights.flatten()[:num_elements]}\")\n",
    "    print(f\"First {num_elements} biases: {layer.biases.flatten()[:num_elements]}\")\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    print(predictions[:10])\n",
    "    print(Y[:10])\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_derivative(y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_loss(y_true, y_pred, epsilon=1e-15):\n",
    "        # Clip the predictions to avoid log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_loss_derivative(y_true, y_pred, epsilon=1e-15):\n",
    "        # Clip the predictions to avoid division by 0\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Reshape:\n",
    "    def __init__(self, output_shape):\n",
    "        self.input_shape = None\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = input.shape[1:]\n",
    "        self.batch_size = input.shape[0]\n",
    "        if type(self.output_shape) is int:\n",
    "            self.output_shape = (self.output_shape,)\n",
    "        return np.reshape(input, (self.batch_size, *self.output_shape))\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, (self.batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MaxPooling2D:\n",
    "    def __init__(self, pool_size=(2, 2), stride=(2, 2)):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.batch_size, self.input_height, self.input_width, self.input_channels = input.shape\n",
    "        self.input = input\n",
    "\n",
    "        self.output_height = (self.input_height - self.pool_size[0]) // self.stride[0] + 1\n",
    "        self.output_width = (self.input_width - self.pool_size[1]) // self.stride[1] + 1\n",
    "\n",
    "        # Initialize output tensor\n",
    "        output = np.zeros((self.batch_size, self.output_height, self.output_width, self.input_channels))\n",
    "\n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                h_start = i * self.stride[0]\n",
    "                h_end = h_start + self.pool_size[0]\n",
    "                w_start = j * self.stride[1]\n",
    "                w_end = w_start + self.pool_size[1]\n",
    "\n",
    "                region = self.input[:, h_start:h_end, w_start:w_end, :]\n",
    "                output[:, i, j, :] = np.max(region, axis=(1, 2))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        input_gradient = np.zeros_like(self.input)\n",
    "\n",
    "        for i in range(self.output_height):\n",
    "            for j in range(self.output_width):\n",
    "                h_start = i * self.stride[0]\n",
    "                h_end = h_start + self.pool_size[0]\n",
    "                w_start = j * self.stride[1]\n",
    "                w_end = w_start + self.pool_size[1]\n",
    "\n",
    "                region = self.input[:, h_start:h_end, w_start:w_end, :]\n",
    "                max_mask = (region == np.max(region, axis=(1, 2), keepdims=True))\n",
    "                \n",
    "                input_gradient[:, h_start:h_end, w_start:w_end, :] += max_mask * output_gradient[:, i, j, :][:, None, None, :]\n",
    "        \n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras import layers, models, optimizers\n",
    "\n",
    "# print(X_train.shape)\n",
    "# X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 32, 32, 1))\n",
    "# X_test_reshaped = np.reshape(X_dev, (X_dev.shape[0], 32, 32, 1))\n",
    "\n",
    "# # Build the LeNet-5 model\n",
    "# model = models.Sequential([\n",
    "#     layers.InputLayer(input_shape=(32, 32, 1)),\n",
    "    \n",
    "#     # C1 Convolutional Layer\n",
    "#     layers.Conv2D(filters=6, kernel_size=(5, 5), activation='tanh', padding='same'),\n",
    "    \n",
    "#     # S2 Subsampling Layer (Average Pooling)\n",
    "#     layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "#     # C3 Convolutional Layer\n",
    "#     layers.Conv2D(filters=16, kernel_size=(5, 5), activation='tanh'),\n",
    "    \n",
    "#     # S4 Subsampling Layer (Average Pooling)\n",
    "#     layers.MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "    \n",
    "#     # C5 Fully Connected Convolutional Layer\n",
    "#     layers.Conv2D(filters=120, kernel_size=(5, 5), activation='tanh'),\n",
    "    \n",
    "#     # Flatten the output for the fully connected layers\n",
    "#     layers.Flatten(),\n",
    "    \n",
    "#     # F6 Fully Connected Layer\n",
    "#     layers.Dense(units=84, activation='tanh'),\n",
    "    \n",
    "#     # Output Layer\n",
    "#     layers.Dense(units=10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Summary of the model\n",
    "# history = model.fit(X_train_reshaped, Y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "[3 0 4 1 9 2 1 3 1 4]\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "0.904\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_reshaped)\n",
    "y_pred = get_predictions(y_pred)\n",
    "print(get_accuracy(y_pred, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann = NeuralNetwork()\n",
    "# ann.add_layer(DenseLayer(10, init=\"Xavier\"))\n",
    "# ann.add_layer(Tanh())\n",
    "# ann.add_layer(DenseLayer(10, init=\"Xavier\"))\n",
    "# ann.add_layer(Softmax(softmax_logloss=True))\n",
    "# ann.compile(loss=\"log_loss\")\n",
    "# ann.train(X_train, Y_train, epochs=500, learning_rate=1, batch_size=50, softmax_logloss=True, isOne_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.32583533809614645\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[0 7 1 1 4 9 4 3 4 8]\n",
      "Epoch 0, accuracy: 0.11133333333333334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[269], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m ann\u001b[38;5;241m.\u001b[39madd_layer(Softmax(softmax_logloss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     16\u001b[0m ann\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mann\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_logloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misOne_hot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[262], line 60\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, X, Y, epochs, learning_rate, batch_size, softmax_logloss, isOne_hot)\u001b[0m\n\u001b[0;32m     57\u001b[0m         grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_derivative(y_batch, output)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m error\n\u001b[0;32m     63\u001b[0m total_error \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m n_batches\n",
      "Cell \u001b[1;32mIn[262], line 30\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, output):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 30\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[251], line 67\u001b[0m, in \u001b[0;36mConvolutional2D.backward\u001b[1;34m(self, output_gradient, learning_rate)\u001b[0m\n\u001b[0;32m     61\u001b[0m upsampled_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\n\u001b[0;32m     62\u001b[0m     (output_gradient\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     63\u001b[0m     (output_gradient\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m ))\n\u001b[0;32m     65\u001b[0m upsampled_gradient[::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, ::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride] \u001b[38;5;241m=\u001b[39m output_gradient[i, :, :, k]\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     68\u001b[0m     kernels_gradient[:, :, c, k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mcorrelate2d(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput[i, :, :, c], upsampled_gradient, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[0;32m     71\u001b[0m     input_gradient[i, :, :, c] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mconvolve2d(\n\u001b[0;32m     72\u001b[0m         upsampled_gradient, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[:, :, c, k], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     73\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ann = NeuralNetwork()\n",
    "ann.add_layer(Reshape((32, 32, 1)))\n",
    "ann.add_layer(Convolutional2D(5, 5))\n",
    "ann.add_layer(Tanh())\n",
    "ann.add_layer(MaxPooling2D(pool_size=(2,2), stride=(2,2)))\n",
    "ann.add_layer(Convolutional2D(5, 16))\n",
    "ann.add_layer(Tanh())\n",
    "ann.add_layer(MaxPooling2D(pool_size=(2,2), stride=(2,2)))\n",
    "ann.add_layer(Convolutional2D(5, 120))\n",
    "ann.add_layer(Tanh())\n",
    "ann.add_layer(Reshape(120))\n",
    "ann.add_layer(DenseLayer(84, init=\"Xavier\"))\n",
    "ann.add_layer(Tanh())\n",
    "ann.add_layer(DenseLayer(10, init=\"Xavier\"))\n",
    "ann.add_layer(Softmax(softmax_logloss=True))\n",
    "ann.compile(loss=\"log_loss\")\n",
    "ann.train(X_train, Y_train, epochs=10, learning_rate=0.1, batch_size=32, softmax_logloss=True, isOne_hot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
